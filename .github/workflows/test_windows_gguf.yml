name: Test Windows GGUF Conversion

on:
  workflow_dispatch:
  pull_request:
    paths:
      - '.github/workflows/test_windows_gguf.yml'

jobs:
  test_gguf_conversion:
    name: Test convert_hf_to_gguf.py on Windows
    runs-on: windows-latest
    steps:
      - name: Checkout safetensors
        uses: actions/checkout@v4

      - name: Install Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: |
          pip install torch --index-url https://download.pytorch.org/whl/cpu
          pip install safetensors transformers sentencepiece protobuf pyyaml gguf "huggingface_hub[hf_xet]"
        shell: bash

      - name: Clone llama.cpp
        run: |
          git clone --depth 1 https://github.com/ggml-org/llama.cpp.git
        shell: bash

      - name: Download Qwen model
        run: |
          python -c "from huggingface_hub import snapshot_download; snapshot_download('Qwen/Qwen2.5-7B', local_dir='./qwen-7b')"
        shell: bash

      - name: Run convert_hf_to_gguf.py with Qwen model
        run: |
          cd llama.cpp
          python convert_hf_to_gguf.py --outfile test.gguf ../qwen-7b
        shell: bash
